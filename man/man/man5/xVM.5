'\" te
.\" Copyright (c) 2009, Sun Microsystems, Inc. All Rights Reserved
.\" The contents of this file are subject to the terms of the Common Development and Distribution License (the "License").  You may not use this file except in compliance with the License.
.\" You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE or http://www.opensolaris.org/os/licensing.  See the License for the specific language governing permissions and limitations under the License.
.\" When distributing Covered Code, include this CDDL HEADER in each file and include the License file at usr/src/OPENSOLARIS.LICENSE.  If applicable, add the following below this CDDL HEADER, with the fields enclosed by brackets "[]" replaced with your own identifying information: Portions Copyright [yyyy] [name of copyright owner]
.TH xVM 5 "14 Jan 2009" "SunOS 5.11" "Standards, Environments, and Macros"
.SH NAME
xVM, xvm \- Solaris x86 virtual machine monitor
.SH DESCRIPTION
.sp
.LP
The Solaris xVM software (hereafter xVM) is a virtualization system, or \fBvirtual machine monitor\fR, for the Solaris operating system on x86 systems. Solaris xVM enables you to run multiple virtual machines simultaneously on a single physical machine, often referred to as the \fBhost\fR. Each virtual machine, known as a \fBdomain\fR, runs its own complete and distinct operating system instance, which includes its own I/O devices. This differs from zone-based virtualization, in which all zones shares the same kernel. (See \fBzones\fR(5).)
.sp
.LP
Each domain has a name and a UUID. Domains can be renamed but typically retain the same UUID. A domain ID is an integer that is specific to a running instance. It changes on every boot of the guest domain. Non-running domains do not have a domain ID.
.sp
.LP
The xVM hypervisor is responsible for controlling and executing each of the domains and runs with full privileges. xVM also includes control tools for management of the domains. These tools run under a specialized control domain, often referred to as \fBdomain 0\fR.
.sp
.LP
To run Solaris xVM, select the entry labelled \fBSolaris xVM\fR in the \fBgrub\fR(5) menu at boot time. This boots the hypervisor, which in turn boots the control domain. The control domain is a version of Solaris modified to run under the xVM hypervisor. At the point at which the control domain is running, the control tools are enabled. In most other respects, the domain 0 instance behaves just like a "normal" instance of the Solaris operating system.
.sp
.LP
The xVM hypervisor delegates control of the physical devices present on the machine to domain 0. Thus, by default, only domain 0 has access to such devices. The other \fBguest\fR domains running on the host are presented with virtualized devices with which they interact as they would physical devices.
.sp
.LP
The xVM hypervisor schedules running domains (including domain 0) onto the set of physical CPUs as configured. The xVM scheduler is constrained by domain configuration (the number of virtual CPUs allocated, and so forth) as well as any run-time configuration, such as pinning virtual CPUs to physical CPUs.
.sp
.LP
The default domain scheduler in xVM is the credit scheduler. This is a work-conserving, fair-share domain scheduler; it balances virtual CPUs of domains across the allowed set of physical CPUs according to workload.
.sp
.LP
xVM supports two modes of virtualization. In the first, the operating system is aware that it is running under xVM. This mode is called \fBparavirtualization\fR. In the second mode, called \fBfully virtualized\fR, the guest operating system is not aware that it is running under xVM. A fully virtualized guest domain is sometimes referred to as a \fBHardware-assisted Virtual Machine\fR (HVM). A variation on a Hardware-assisted Virtual Machine is to use paravirtualized drivers for improved performance. This variation is abbreviated as HVM + PV.
.sp
.LP
With paravirtualization, each device, such as a networking interface, is presented as a fully virtual interface, and specific drivers are required for it. Each virtual device is associated with a physical device and the driver is split into two. A \fBfrontend\fR driver runs in the guest domain and communicates over a virtual data interface to a \fBbackend\fR driver. The \fBbackend\fR driver currently runs in domain 0 and communicates with both the frontend driver and the physical device underneath it. Thus, a guest domain can make use of a network card on the host, store data on a host disk drive, and so forth.
.sp
.LP
Solaris xVM currently supports two main split drivers used for I/O. Networking is done by means of the \fBxnb\fR (xVM Networking Backend) drivers. Guest domains, whether Solaris or another operating system, use \fBxnb\fR to transmit and receive networking traffic. Typically, a physical NIC is used for communicating with the guest domains, either shared or dedicated. Solaris xVM provides networking access to guest domains by means of MAC-based virtual network switching.
.sp
.LP
Block I/O is provided by the \fBxdb\fR (xVM Disk Backend) driver, which provides virtual disk access to guest domains. In the control domain, the disk storage can be in a file, a ZFS volume, or a physical device.
.sp
.LP
Using ZFS volumes as the virtual disk for your guest domains allows you to take a snapshot of the storage. As such, you can keep known-good snapshots of the guest domain OS installation, and revert the snapshot (using \fBzfs rollback\fR) if the domain has a problem. The \fBzfs clone\fR command can be used for quick provisioning of new domains. For example, you might install Solaris as a guest domain, run \fBsys-unconfig\fR(1M), then clone that disk image for use in new Solaris domains. Installing Solaris in this way requires only a configuration step, rather than a full install.
.sp
.LP
When running as a guest domain, Solaris xVM uses the \fBxnf\fR and \fBxdf\fR drivers to talk to the relevant backend drivers. In addition to these drivers, the Solaris console is virtualized when running as a guest domain; this driver interacts with the \fBxenconsoled\fR(1M) daemon running in domain 0 to provide console access.
.sp
.LP
A given system can have both paravirtualized and fully virtualized domains running simultaneously. The control domain must always run in paravirtualized mode, because it must work closely with the hypervisor layer.
.sp
.LP
As guest domains do not share a kernel, xVM does not require that every guest domain be Solaris. For paravirtualized mode and for all types of operating systems, the only requirement is that the operating system be modified to support the virtual device interfaces.
.sp
.LP
Fully-virtualized guest domains are supported under xVM with the assistance of virtualization extensions available on some x86 CPUs. These extensions must be present and enabled; some BIOS versions disable the extensions by default.
.sp
.LP
In paravirtualized mode, Solaris identifies the platform as \fBi86xpv\fR, as seen in the return of the following \fBuname\fR(1) command:
.sp
.in +2
.nf
% \fBuname -i\fR
i86xpv
.fi
.in -2
.sp

.sp
.LP
Generally, applications do not need to be aware of the platform type. It is recommended that any ISA identification required by an application use the \fB-p\fR option (for ISA or processor type) to \fBuname\fR, rather than the \fB-i\fR option, as shown above. On x86 platforms, regardless of whether Solaris is running under xVM, \fBuname\fR \fB-p\fR always returns \fBi386\fR.
.sp
.LP
You can examine the \fBdomcaps\fR driver to determine whether a Solaris instance is running as domain 0:
.sp
.in +2
.nf
# \fBcat /dev/xen/domcaps\fR
control_d
.fi
.in -2
.sp

.sp
.LP
Note that the \fBdomcaps\fR file might also contain additional information. However, the first token is always \fBcontrol_d\fR when running as domain 0.
.sp
.LP
xVM hosts provide a service management facility (SMF) service (see \fBsmf\fR(5)) with the FMRI:
.sp
.in +2
.nf
svc:/system/xvm/domains:default
.fi
.in -2
.sp

.sp
.LP
\&...to control auto-shutdown and auto-start of domains. By default, all running domains are shutdown when the host is brought down by means of \fBinit 6\fR or a similar command. This shutdown is analogous to entering:
.sp
.in +2
.nf
# \fBvirsh shutdown \fImydomain\fR\fR
.fi
.in -2
.sp

.sp
.LP
A domain can have the setting:
.sp
.in +2
.nf
on_xend_stop=ignore
.fi
.in -2
.sp

.sp
.LP
\&...in which case the domain is not shut down even if the host is. Such a setting is the effective equivalent of:
.sp
.in +2
.nf
# \fBvirsh destroy \fImydomain\fR\fR
.fi
.in -2
.sp

.sp
.LP
If a domain has the setting:
.sp
.in +2
.nf
on_xend_start=start
.fi
.in -2
.sp

.sp
.LP
\&...then the domain is automatically booted when the xVM host boots. Disabling the SMF service by means of \fBsvcadm\fR(1M) disables this behavior for all domains.
.sp
.LP
Solaris xVM is partly based on the work of the open source Xen community.
.SS "Control Tools"
.sp
.LP
The control tools are the utilities shipped with Solaris xVM that enable you to manage xVM domains. These tools interact with the daemons that support xVM: \fBxend\fR(1M), \fBxenconsoled\fR(1M), and \fBxenstored\fR(1M), each described in its own man page. The daemons are, in turn, managed by the SMF.
.sp
.LP
You install new guest domains with the command line \fBvirt-install\fR(1M) or the graphical interface, \fBvirt-manager\fR.
.sp
.LP
The main interface for command and control of both xVM and guest domains is \fBvirsh\fR(1M). Users should use \fBvirsh\fR(1M) wherever possible, as it provides a generic and stable interface for controlling virtualized operating systems. However, some xVM operations are not yet implemented by \fBvirsh\fR. In those cases, the legacy utility \fBxm\fR(1M) can be used for detailed control.
.sp
.LP
The configuration of each domain is stored by \fBxend\fR(1M) after it has been created by means of \fBvirt-install\fR, and can be viewed using the \fBvirsh dumpxml\fR or \fBxm list\fR \fB-l\fR commands. Direct modification of this configuration data is not recommended; the command-line utility interfaces should be used instead.
.sp
.LP
Solaris xVM supports live migration of domains by means of the \fBxm migrate\fR command. This allows a guest domain to keep running while the host running the domain transfers ownership to another physical host over the network. The remote host must also be running xVM or a compatible version of Xen, and must accept migration connections from the current host (see \fBxend\fR(1M)).
.sp
.LP
For migration to work, both hosts must be able to access the storage used by the \fBdomU\fR (for example, over iSCSI or NFS), and have enough resources available to run the migrated domain. Both hosts must currently reside on the same subnet for the guest domain networking to continue working properly. Also, both hosts should have similar hardware. For example, migrating from a host with AMD processors to one with Intel processors can cause problems.
.sp
.LP
Note that the communication channel for migration is not a secure connection.
.SH GLOSSARY
.sp
.ne 2
.mk
.na
\fB\fBbackend\fR\fR
.ad
.sp .6
.RS 4n
Describes the other half of a virtual driver from the \fBfrontend\fR driver. The backend driver provides an interface between the virtual device and an underlying physical device.
.RE

.sp
.ne 2
.mk
.na
\fB\fBcontrol domain\fR\fR
.ad
.sp .6
.RS 4n
The special guest domain running the control tools and given direct hardware access by the hypervisor. Often referred to as \fBdomain 0\fR.
.RE

.sp
.ne 2
.mk
.na
\fB\fBdomain 0\fR\fR
.ad
.sp .6
.RS 4n
See \fBcontrol domain\fR.
.RE

.sp
.ne 2
.mk
.na
\fB\fBfrontend\fR\fR
.ad
.sp .6
.RS 4n
Describes a virtual device and its associated driver in a guest domain. A frontend driver communicates with a \fBbackend\fR driver hosted in a different guest domain.
.RE

.sp
.ne 2
.mk
.na
\fB\fBfull virtualization\fR\fR
.ad
.sp .6
.RS 4n
Running an unmodified operating system with hardware assistance.
.RE

.sp
.ne 2
.mk
.na
\fB\fBguest domain\fR\fR
.ad
.sp .6
.RS 4n
A virtual machine instance running on a host. Unless the guest is domain 0, such a domain is also called a \fBdomain-U\fR, where \fBU\fR stands for "unprivileged".
.RE

.sp
.ne 2
.mk
.na
\fB\fBhost\fR\fR
.ad
.sp .6
.RS 4n
The physical machine running xVM.
.RE

.sp
.ne 2
.mk
.na
\fB\fBHardware-assisted Virtual Machine\fR (HVM)\fR
.ad
.sp .6
.RS 4n
A fully-virtualized guest domain.
.RE

.sp
.ne 2
.mk
.na
\fB\fBnode\fR\fR
.ad
.sp .6
.RS 4n
The name used by the \fBvirsh\fR(1M) utility for a host.
.RE

.sp
.ne 2
.mk
.na
\fB\fBvirtual machine monitor\fR (VMM)\fR
.ad
.sp .6
.RS 4n
Hypervisor software, such as xVM, that manages multiple domains.
.RE

.SH SEE ALSO
.sp
.LP
\fBuname\fR(1), \fBdladm\fR(1M), \fBsvcadm\fR(1M), \fBsys-unconfig\fR(1M), \fBvirsh\fR(1M), \fBvirt-install\fR(1M), \fBxend\fR(1M), \fBxenconsoled\fR(1M), \fBxenstored\fR(1M), \fBxm\fR(1M), \fBzfs\fR(1M), \fBattributes\fR(5), \fBgrub\fR(5), \fBlive_upgrade\fR(5), \fBsmf\fR(5), \fBzones\fR(5)
.SH NOTES
.sp
.LP
Any physical Ethernet datalink (as shown by \fBdladm show-phys\fR) can be used to network guest domains.
